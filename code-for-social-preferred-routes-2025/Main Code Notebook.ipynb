{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ks_2samp\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.legend_handler import HandlerTuple\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "\n",
    "import Analysis as lyze\n",
    "import Archive as arch\n",
    "import GlobalSim\n",
    "import LocalSim\n",
    "import dataHandling as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real World Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directory & File Name Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_directory = os.getcwd()  # Replace with your own directory path\n",
    "hashtags = arch.list_folder(main_directory)\n",
    "\n",
    "# pattern of dates in the files' names\n",
    "pattern = r'_(\\d+-+\\d+_+\\d+-+\\d+)' \n",
    "dates = arch.extract_patterns_from_filenames(os.path.join(main_directory,hashtags[0]), pattern)\n",
    "\n",
    "hashtag_sets = [{'hashtag': np.arange(0,len(hashtags)), 'label':'all'}]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Combining temporal networks\n",
    "*(This section can be skipped if already done once.)*\n",
    "\n",
    "\n",
    "In this section, we combine the networks temporally, such that from the 9 initial timespans of the retweet data, we get only 2 timespans from the aggregation of the first the first 4 timespans and the second 5 timespans. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this part if the combined networks have already been saved \n",
    "# and jump to the next cell to read those networks\n",
    "for hashtag in hashtags:\n",
    "    print(f\"{hashtag} is under process:\")\n",
    "    data.compose_temporal_union(main_directory, \"1sthalf\", [hashtag], dates[:4], save=True)\n",
    "    print(f\"First half of {hashtag} completed.\")\n",
    "    data.compose_temporal_union(main_directory, \"2ndhalf\", [hashtag], dates[5:], save=True)\n",
    "    print(f\"Second half of {hashtag} completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Combined Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the combined data\n",
    "first_half_data = arch.mass_read_graphs(hashtags, main_directory, '1sthalf')\n",
    "second_half_data = arch.mass_read_graphs(hashtags, main_directory, '2ndhalf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preference Existence Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "overlap_results, similarity_results = lyze.parallel_similarity_overlap(first_half_data,\n",
    "                                    second_half_data,mode='both', weighted_overlap=True,\n",
    "                                    overlap=True, similarity=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Preferential Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Underlying Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation\n",
    "Start a network from scratch and evolve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.path.join(os.getcwd(), 'Sim_results')\n",
    "\n",
    "N = 400  # Number of nodes\n",
    "T = 5000  # Number of timesteps\n",
    "m = 10 #number of new links added at each timesteps\n",
    "m0 = 20  # Number of nodes with non-zero degree at t=0\n",
    "x = 3  # Link increment-proportion percentage\n",
    "steps = 25 #adjacency matrices will be saved at every other 'steps'\n",
    "save_steps = np.arange(0,T,steps)  # Timesteps to save adjacency matrices\n",
    "init_type = 'random' # could also be 'path', see GlobalSim package\n",
    "update_mode = 'increment' #could also be 'proportional', see GlobalSim package\n",
    "\n",
    "output_dir = arch.create_folder_in_wd('global2_'+init_type[0:4]+'_'+update_mode[0:3]+'_'+arch.generate_magic_number(),\n",
    "                                    directory) # Directory to save snapshots\n",
    "sim_params = {'N':N, 'T':T, 'm0':m0, 'm':m,'x':x, 'steps':steps, 'save_steps':np.array(save_steps),\n",
    "              'output_dir':str(\"r'\")+output_dir+str(\"'\"),\n",
    "              'update_mode':str(\"'\")+update_mode+str(\"'\"),\n",
    "              'init_type':str(\"'\")+init_type+str(\"'\")}\n",
    "\n",
    "GlobalSim.simulate_global_preference(N, T, m, x, save_steps, output_dir, update_mode, init_type)\n",
    "\n",
    "arch.save_function_arguments(os.path.join(output_dir,'Parameters.txt'), sim_params)\n",
    "print(f\"Saved {len(save_steps)} adjacency matrices.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue Simulation\n",
    "Continue evolving the network from a saved adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"manually input the properties of the adjacency matrices \n",
    "or copy from 'parameter' text file in the simulation folder\"\"\"\n",
    "\n",
    "t0 = 1990 #from when to begin continuing simulation\n",
    "T = 2000 #timesetps to continue\n",
    "m = 3 #nummber of links added to the graph at each timesteps\n",
    "update_mode = 'increment'\n",
    "x= 2 #update parameter based on update_mode\n",
    "steps= 10\n",
    "save_steps = np.arange(t0,T,steps)\n",
    "output_dir = os.getcwd() #change this directory for your purpose\n",
    "\n",
    "#continue simulation\n",
    "adj_matrix = np.load(os.path.join(output_dir, f'adjacency_matrix_t{t0}.npy'))\n",
    "GlobalSim.continue_global_simulation(adj_matrix, t0, T, m, x, save_steps, output_dir, update_mode )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading saved data\n",
    "If the underlying network is already simulated, just load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N= 400\n",
    "T= 5000\n",
    "m0= 20\n",
    "m= 10\n",
    "x= 3\n",
    "steps= 25\n",
    "output_dir= r'c:\\Users\\ROZHIN\\Desktop\\New folder (3)\\Code\\00 RE\\Sim_results\\global2_rand_inc_58-10-03-06-25'\n",
    "update_mode= 'increment'\n",
    "init_type= 'random'\n",
    "\n",
    "\n",
    "#::::::::::::::::::::::::::::: select the evolution timestep you wish to analyze\n",
    "t =2000\n",
    "directory1 = os.path.join(output_dir,'adjacency_matrix_t'+str(t)+'.npy')\n",
    "global_adjacency_matrix = np.load(directory1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retweet Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(global_adjacency_matrix)\n",
    "global_rt_sim_ens = 120 #the number of simulation repetition (ensemble)\n",
    "global_rt_noise_base = 0.3\n",
    "\n",
    "#In simulation, since no evolution is happening, \n",
    "# only those with non-zero degree can participate in sharing\n",
    "global_init_nodes = [i for i in range(len(global_adjacency_matrix)) if np.sum(global_adjacency_matrix[i,:])!=0]\n",
    "\n",
    "global_rt_sim_paths = []\n",
    "for i in tqdm(range(global_rt_sim_ens)):\n",
    "    global_rt_noise = np.abs(global_rt_noise_base + np.random.normal(0,0.05)) #optional to add a deviation in noise\n",
    "    rand_coeff =  np.random.choice(np.arange(1,20)) #random coefficient to increase the retweet-network size ratio\n",
    "    init = np.random.choice(global_init_nodes) #only choose from those nodes who can be initiators\n",
    "    paths, _ = lyze.simulate_retweet(global_adjacency_matrix, N*rand_coeff,global_rt_noise,init_node=init)\n",
    "    global_rt_sim_paths.append(paths)\n",
    "\n",
    "#saving the results\n",
    "file_name = rf'pickles\\global_retweet_simulation_N_{N}_t{t}_{global_rt_sim_ens}ens_{global_rt_noise_base}noise_'+arch.generate_magic_number()+'.pkl'\n",
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump(global_rt_sim_paths, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Preferential Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The underlying netwrok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation\n",
    "Start a network from scratch and evolve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m = 10 #number of new links added at each timesteps\n",
    "m0 = 20  # Number of nodes with non-zero degree at t=0\n",
    "x = 3  # Link increment-proportion percentage\n",
    "steps = 25 #adjacency matrices will be saved at every other 'steps'\n",
    "save_steps = np.arange(0,T,steps)  # Timesteps to save adjacency matrices\n",
    "init_type = 'random' # could also be 'path', see LocalSim package\n",
    "update_mode = 'increment' #could also be 'proportional', see LocalSim package\n",
    "\n",
    "N = 400 # Number of nodes\n",
    "T = 10000  # Number of timesteps\n",
    "m = 5  # Number of new links added at each timesteps\n",
    "x = 50 # Link increment-proportion percentage\n",
    "steps = 5 # Adjacency matrices will be saved at every other 'steps'\n",
    "save_steps = np.arange(0,T,steps)  # Timesteps to save adjacency matrices\n",
    "update_mode = 'increment'\n",
    "distribution = \"gaussian\"\n",
    "params = {\"mean\": 6, \"std\": 2}\n",
    "output_dir = arch.create_folder_in_wd(os.path.join(main_directory,'Sim_results',\n",
    "                            'local_'+distribution[0:3]+'_'+update_mode[0:3]+'_'+arch.generate_magic_number())) # Directory to save snapshots\n",
    "sim_params ={'N':N, 'T':T, 'm':m,'x':x, 'steps':steps, 'save_steps':np.array(save_steps), 'output_dir':str(\"r'\")+output_dir+str(\"'\"),\n",
    "              'update_mode':str(\"'\")+update_mode+str(\"'\"),\n",
    "            'distribution': str(\"'\")+distribution+str(\"'\"), 'params':params}\n",
    "\n",
    "LocalSim.simulate_local_preference(N, T, m, x, save_steps, output_dir, update_mode, distribution, params)\n",
    "\n",
    "arch.save_function_arguments(os.path.join(output_dir,'Parameters.txt'), sim_params)\n",
    "print(f\"Saved {len(save_steps)} adjacency matrices.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#:::::::::::::::::::::::::::::: insert the 'parameters' file's contents here\n",
    "N= 400\n",
    "T= 10000\n",
    "m= 5\n",
    "x= 50\n",
    "steps= 5\n",
    "# save_steps= [   0    5   10 ... 9985 9990 9995]\n",
    "output_dir= r'C:\\Users\\ROZHIN\\Desktop\\New folder (3)\\Code\\00 RE\\Sim_results\\local'\n",
    "update_mode= 'increment'\n",
    "distribution= 'gaussian'\n",
    "params= {'mean': 6, 'std': 2}\n",
    "\n",
    "\n",
    "\n",
    "#:::::::::::::::::::::::::::: select the evolution timestep you wish to analyze\n",
    "t =800\n",
    "directory1 = os.path.join(output_dir,'adjacency_matrix_t'+str(t)+'.npy')\n",
    "local_adjacency_matrix = np.load(directory1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retweet Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(local_adjacency_matrix)\n",
    "local_rt_sim_ens = 120 #the number of simulation repetition (ensemble)\n",
    "local_rt_noise_base = 0.1\n",
    "\n",
    "# since in local simulation, all nodes have non-zero degree, we randomly pick almost a quarter of\n",
    "# the present nodes as the initiating nodes of a retweet thread\n",
    "local_init_nodes = np.random.choice(N, int(N/4), replace=False)\n",
    "\n",
    "steps_taken = []\n",
    "local_rt_sim_paths = []\n",
    "for i in tqdm(range(local_rt_sim_ens)):\n",
    "    local_rt_noise = np.abs(local_rt_noise_base + np.random.normal(0,0.05)) #gaussian_like_random\n",
    "    rand_coeff =  np.random.choice(np.arange(10,200)) #int(np.random.choice(ratios)\n",
    "    init = np.random.choice(local_init_nodes)\n",
    "    paths, steps = lyze.simulate_retweet(local_adjacency_matrix, N*rand_coeff, local_rt_noise, init_node = init)\n",
    "    local_rt_sim_paths.append(paths)\n",
    "    steps_taken.append(steps)\n",
    "\n",
    "\n",
    "#saving the repickles/sults\n",
    "file_name = f'pickles/local_retweet_simulation_t{t}_{local_rt_sim_ens}ens_{local_rt_noise}noise_'+arch.generate_magic_number()+'.pkl'\n",
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump(local_rt_sim_paths, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Preference Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retweet Simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "rand_adjacency_matrix = (np.ones((N,N)) - np.eye(N))/(N-1)\n",
    "\n",
    "rand_rt_sim_ens = 120\n",
    "rand_rt_noise = 0\n",
    "\n",
    "init_nodes = np.random.choice(N,int(N/4), replace=False)\n",
    "\n",
    "rand_rt_sim_paths = []\n",
    "steps_taken = []\n",
    "for i in tqdm(range(rand_rt_sim_ens)):\n",
    "    rand_coeff = np.random.randint(1,5) \n",
    "    init = np.random.choice(init_nodes) #np.random.choice(init_nodes)\n",
    "    paths, steps = lyze.simulate_retweet(rand_adjacency_matrix, N*rand_coeff, rand_rt_noise, init_node = init)\n",
    "    rand_rt_sim_paths.append(paths)\n",
    "    steps_taken.append(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing All Cases together\n",
    "In this section, we apply the **Modified Weighted Jaccard Indec** and **Cosine Similatiy** measures to the four cases present:\n",
    "1. Real Data\n",
    "2. Global Preference Model\n",
    "3. Local Preference Model\n",
    "4. No Preference Model\n",
    "\n",
    "And plot the results, before assessing the distribution of the results of these measures on pairs of networks using KS test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified Weighted Jaccard Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loadig Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real data\n",
    "# Previously calclulated Modified Weighted Jaccard Index (mentioned as overlap for convinience)\n",
    "with open(r'New_1sthalf_2ndhalf_pair_overlap_coeff_all_both_intersection_weighted_23-15-30-01-25.pkl', 'rb') as f:\n",
    "    overlap_results = pickle.load(f)\n",
    "\n",
    "overlaps = []\n",
    "for index, (key, value) in enumerate(overlap_results.items()):\n",
    "    ij = key.split('_')\n",
    "    i = int(ij[0])\n",
    "    j = int(ij[1])\n",
    "    overlaps.append(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global simulation\n",
    "\n",
    "\"\"\" To read simulation results from a file, uncomment this part\"\"\"\n",
    "# simulation_dir = os.getcw() #change this directory for your purpose\n",
    "# with open(simulation_dir, 'rb') as f:\n",
    "#     global_rt_sim_paths = pickle.load(f)\n",
    "\n",
    "global_overlap_results = []\n",
    "group_sims = 6\n",
    "len_group = int(len(global_rt_sim_paths)/group_sims)\n",
    "for i in range(group_sims):\n",
    "    group_overlap_results = []\n",
    "    for path1, path2 in combinations(global_rt_sim_paths[i*len_group:(i+1)*len_group], 2):\n",
    "        group_overlap_results.append(lyze.overlap_coefficient(path1, path2))\n",
    "\n",
    "    global_overlap_results.append(group_overlap_results)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local simulation\n",
    "\n",
    "\"\"\" To read simulation results from a file, uncomment this part\"\"\"\n",
    "# simulation_dir = os.getcw() #change this directory for your purpose\n",
    "# with open(simulation_dir, 'rb') as f:\n",
    "#     local_rt_sim_paths = pickle.load(f)\n",
    "\n",
    "local_overlap_results = []\n",
    "group_sims = 6\n",
    "len_group = int(len(local_rt_sim_paths)/group_sims)\n",
    "for i in range(group_sims):\n",
    "    group_overlap_results = []\n",
    "    for path1, path2 in combinations(local_rt_sim_paths[i*len_group:(i+1)*len_group], 2):\n",
    "        group_overlap_results.append(lyze.overlap_coefficient(path1, path2))\n",
    "\n",
    "    local_overlap_results.append(group_overlap_results)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random data\n",
    "\n",
    "#---------------- Simulation of retweet\n",
    "N = 50\n",
    "rand_adjacency_matrix = (np.ones((N,N)) - np.eye(N))/(N-1)\n",
    "\n",
    "node_num = len(rand_adjacency_matrix) \n",
    "rand_rt_sim_ens = 120\n",
    "rand_rt_noise = 0\n",
    "init_nodes = np.arange(0,node_num,5)\n",
    "\n",
    "rand_rt_sim_paths = []\n",
    "steps_taken = []\n",
    "for i in tqdm(range(rand_rt_sim_ens)):\n",
    "    rand_coeff = np.random.randint(1,5) \n",
    "    init = np.random.choice(init_nodes) #np.random.choice(init_nodes)\n",
    "    paths, steps = lyze.simulate_retweet(rand_adjacency_matrix, node_num*rand_coeff, rand_rt_noise, init_node = init)\n",
    "    rand_rt_sim_paths.append(paths)\n",
    "    steps_taken.append(steps)\n",
    "\n",
    "#---------------- calculating the modified jaccard index\n",
    "rand_overlap_results = []\n",
    "m = 6\n",
    "len_group = int(len(rand_rt_sim_paths)/m)\n",
    "# print(len_group)\n",
    "for i in range(m):\n",
    "    group_overlap_results = []\n",
    "    taken_sims = rand_rt_sim_paths[i*len_group:(i+1)*len_group]\n",
    "    for path1, path2 in combinations(taken_sims, 2):\n",
    "        group_overlap_results.append(lyze.overlap_coefficient(path1, path2))\n",
    "    rand_overlap_results.append(group_overlap_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sturges(n):\n",
    "    return int(np.log(n)/np.log(2)) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"#1f77b4\",\"#ff7f0e\",\"#2ca02c\",\"#d62728\"]\n",
    "\n",
    "probabilities = []\n",
    "rand_bin = 10\n",
    "bin_num = sturges(len(overlaps))\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "\n",
    "# global:::::::::::::::::::::::::::::::::\n",
    "\"\"\"the middle points\"\"\"\n",
    "y_data, bins = np.histogram(np.mean(global_overlap_results, axis=0), bins=bin_num, density=True)\n",
    "\"\"\"vertical points for each bin\"\"\"\n",
    "points = np.zeros((len(bins) - 1, len(global_overlap_results)))\n",
    "for i in range(len(global_overlap_results)):\n",
    "    points[:, i], _ = np.histogram(global_overlap_results[i], bins=bins, density=True)\n",
    "y_data, bins = np.histogram(np.array(global_overlap_results).flatten(), bins=bin_num, density=True)\n",
    "y_err = np.std(points, axis=1)  # Compute error\n",
    "y_err /= np.sum(y_data)  # Normalize error to the sum of the original points\n",
    "y_data /= np.sum(y_data)  # Normalize\n",
    "probabilities.append(np.sum(y_data))\n",
    "\n",
    "plt.fill_between(bins[:-1], y_data - y_err, y_data + y_err, alpha=0.6, color=colors[0],\n",
    "                label=\"Global Model ± Err\",zorder=3)\n",
    "pr1, = plt.plot(bins[:-1], y_data, lw=3,alpha=0.6,color=colors[0], zorder=3)\n",
    "pr2, = plt.plot(bins[:-1], y_data, 'k',marker=\"v\", ls='dashed', ms=8, mec=\"white\", linewidth=1,\n",
    "                mfc=colors[0], zorder=5,label=\"Global Model ± Err\")\n",
    "\n",
    "\n",
    "# local:::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "y_data, bins = np.histogram(np.mean(local_overlap_results, axis=0), bins=bin_num, density=True)\n",
    "points = np.zeros((len(bins) - 1, len(local_overlap_results)))\n",
    "for i in range(len(local_overlap_results)):\n",
    "    points[:, i], _ = np.histogram(local_overlap_results[i], bins=bins, density=True)\n",
    "y_data, bins = np.histogram(np.array(local_overlap_results).flatten(), bins=bin_num, density=True)\n",
    "y_err = np.std(points, axis=1)  # Compute error\n",
    "y_err /= np.sum(y_data)  # Normalize\n",
    "y_data /= np.sum(y_data)  # Normalize\n",
    "probabilities.append(np.sum(y_data))\n",
    "\n",
    "plt.fill_between(bins[:-1], y_data - y_err, y_data + y_err,color=colors[1], hatch=\"XX\", edgecolor=\"k\",\n",
    "                alpha=0.3, label=\"Local Model ± Err\",zorder=4)\n",
    "pl1, = plt.plot(bins[:-1], y_data, lw=3, alpha=0.6,color=colors[1], zorder=4)\n",
    "pl2, = plt.plot(bins[:-1], y_data, 'k',marker=\"^\", ls='dashdot', ms=8, mec=\"white\",\n",
    "                linewidth=1, mfc=colors[1], zorder=5,label=\"Local Model ± Err\")\n",
    "\n",
    "\n",
    "\n",
    "# random:::::::::::::::::::::::::::::::::::::::::::::::\n",
    "y_data, bins = np.histogram(np.mean(rand_overlap_results, axis=0), bins=rand_bin, density=True)\n",
    "points = np.zeros((len(bins) - 1, len(rand_overlap_results)))\n",
    "for i in range(len(rand_overlap_results)):\n",
    "    points[:, i], _ = np.histogram(rand_overlap_results[i], bins=rand_bin, density=True)\n",
    "y_err = np.std(points, axis=1)  # Compute error\n",
    "y_err /= np.sum(y_data)  # Normalize\n",
    "y_data /= np.sum(y_data)  # Normalize\n",
    "probabilities.append(np.sum(y_data))\n",
    "plt.fill_between(bins[:-1], y_data - y_err, y_data + y_err, alpha=0.3, color=colors[3],\n",
    "                label=\"No Preference ± Err\",zorder=2)\n",
    "pr1, = plt.plot(bins[:-1], y_data, lw=4, zorder=10,alpha=0.6,color=colors[3])\n",
    "pr2, = plt.plot(bins[:-1], y_data, 'k',marker=\"d\", ls='dotted', ms=8, mec=\"white\",\n",
    "                linewidth=1, mfc=colors[3], zorder=10,label=\"No Preference ± Err\")\n",
    "\n",
    "\n",
    "#data:::::::::::::::::::::::::::::::::\n",
    "bin_num = sturges(len(overlaps))\n",
    "y_data, bins = np.histogram(overlaps, bins=bin_num, density=True)\n",
    "y_data /= np.sum(y_data)  # Normalize\n",
    "probabilities.append(np.sum(y_data))\n",
    "pd1, = plt.plot(bins[:-1], y_data, lw=4, color='k', zorder=10,alpha=0.6, label='Retweet Data')\n",
    "pd2, = plt.plot(bins[:-1], y_data, color='white',marker=\"o\", ls=(0,(1,5)), ms=8, mec=\"white\",\n",
    "                linewidth=1, mfc='k', zorder=10,label=\"Retweet Data\")\n",
    "\n",
    "# Titles and Labels\n",
    "plt.xlim(right=1, left=0)\n",
    "plt.ylim(bottom=0)\n",
    "plt.xlabel(r\"$\\tilde{J}_w$\", fontsize=12)\n",
    "plt.ylabel(r\"$P(\\tilde{J}_w)$\", fontsize=12)\n",
    "plt.legend()\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend([ tuple(handles[0:2]), tuple(handles[2:4]), tuple(handles[4:6]), \n",
    "           tuple(handles[6:8])], labels[::2], handlelength=3,\n",
    "           handler_map={tuple: HandlerTuple(ndivide=1)})\n",
    "\n",
    "# plt.savefig(\"Jaccard_index1\"+arch.generate_magic_number()+\".png\",dpi=200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real data\n",
    "with open(r'New_1sthalf_2ndhalf_pair_similarity_all_both_intersection_weighted_08-01-31-01-25.pkl', 'rb') as f:\n",
    "    similarity_results = pickle.load(f)\n",
    "\n",
    "similarities = []\n",
    "for index, (key, value) in enumerate(similarity_results.items()):\n",
    "    similarities.append(value)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global\n",
    "\n",
    "\"\"\" To read simulation results from a file, uncomment this part\"\"\"\n",
    "# simulation_dir = os.getcw() #change this directory for your purpose\n",
    "# with open(simulation_dir, 'rb') as f:\n",
    "#     global_rt_sim_paths = pickle.load(f)\n",
    "\n",
    "global_similarity_results = []\n",
    "group_sims = 6\n",
    "len_group = int(len(global_rt_sim_paths)/group_sims)\n",
    "for i in range(group_sims):\n",
    "    group_similarity_results = []\n",
    "    for path1, path2 in tqdm(combinations(global_rt_sim_paths[i*len_group:(i+1)*len_group], 2)):\n",
    "        _, (mean, std) = lyze.analyze_all_simulation_nodes([path1,path2])\n",
    "        group_similarity_results.append(mean)\n",
    "\n",
    "    global_similarity_results.append(group_similarity_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local\n",
    "\n",
    "\"\"\" To read simulation results from a file, uncomment this part\"\"\"\n",
    "# simulation_dir = os.getcw() #change this directory for your purpose\n",
    "# with open(simulation_dir, 'rb') as f:\n",
    "#     local_rt_sim_paths = pickle.load(f)\n",
    "\n",
    "local_similarity_results = []\n",
    "group_sims = 6\n",
    "len_group = int(len(local_rt_sim_paths)/group_sims)\n",
    "for i in range(group_sims):\n",
    "    group_similarity_results = []\n",
    "    for path1, path2 in tqdm(combinations(local_rt_sim_paths[i*len_group:(i+1)*len_group], 2)):\n",
    "        _, (mean, std) = lyze.analyze_all_simulation_nodes([path1,path2])\n",
    "        group_similarity_results.append(mean)\n",
    "\n",
    "    local_similarity_results.append(group_similarity_results)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random data\n",
    "\n",
    "#-------------------- Simulating Retweet\n",
    "rand_adjacency_matrix = (np.ones((N,N)) - np.eye(N))/(N-1)\n",
    "\n",
    "node_num = len(rand_adjacency_matrix) \n",
    "rand_rt_sim_ens = 120\n",
    "rand_rt_noise = 0\n",
    "#starting node: yes\n",
    "init_nodes = np.arange(0,node_num,5)\n",
    "\n",
    "rand_rt_sim_paths = []\n",
    "steps_taken = []\n",
    "for i in tqdm(range(rand_rt_sim_ens)):\n",
    "    rand_coeff = np.random.randint(1,5) \n",
    "    init = np.random.choice(init_nodes) #np.random.choice(init_nodes)\n",
    "    paths, steps = lyze.simulate_retweet(rand_adjacency_matrix, node_num*rand_coeff, rand_rt_noise, init_node = init)\n",
    "    rand_rt_sim_paths.append(paths)\n",
    "    steps_taken.append(steps)\n",
    "\n",
    "# ------------------- Calculating the cosine similarities\n",
    "\n",
    "rand_similarity_results = []\n",
    "group_sims = 6\n",
    "len_group = int(len(rand_rt_sim_paths)/group_sims)\n",
    "for i in range(group_sims):\n",
    "    group_similarity_results = []\n",
    "    for path1, path2 in tqdm(combinations(rand_rt_sim_paths[i*len_group:(i+1)*len_group], 2)):\n",
    "        _, (mean, std) = lyze.analyze_all_simulation_nodes([path1,path2])\n",
    "        group_similarity_results.append(mean)\n",
    "\n",
    "    rand_similarity_results.append(group_similarity_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"#1f77b4\",\"#ff7f0e\",\"#2ca02c\",\"#d62728\"]\n",
    "\n",
    "bin_num = sturges(len(overlaps))\n",
    "rand_bin = 10\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "\n",
    "# global:::::::::::::::::::::::::::::::::\n",
    "y_data, bins = np.histogram(np.mean(global_similarity_results, axis=0), bins=bin_num, density=True)\n",
    "points = np.zeros((len(bins) - 1, len(global_similarity_results)))\n",
    "for i in range(len(global_similarity_results)):\n",
    "    points[:, i], _ = np.histogram(global_similarity_results[i], bins=bins, density=True)\n",
    "y_data, bins = np.histogram(np.array(global_similarity_results).flatten(), bins=bin_num, density=True)\n",
    "y_err = np.std(points, axis=1)  # Compute error\n",
    "y_err /= np.sum(y_data)  # Normalize\n",
    "y_data /= np.sum(y_data)  # Normalize\n",
    "\n",
    "plt.fill_between(bins[:-1], y_data - y_err, y_data + y_err, alpha=0.4, color=colors[0], label=\"Global Model ± Err\",zorder=3)\n",
    "pr1, = plt.plot(bins[:-1], y_data, lw=3,alpha=0.6,color=colors[0], zorder=3)\n",
    "pr2, = plt.plot(bins[:-1], y_data, 'k',marker=\"v\", ls='dashed', ms=8, mec=\"white\", linewidth=1, mfc=colors[0], zorder=5,label=\"Global Model ± Err\")\n",
    "\n",
    "# local:::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "bin_num = sturges(len(local_overlap_results[0]))\n",
    "y_data, bins = np.histogram(np.mean(local_similarity_results, axis=0), bins=bin_num, density=True)\n",
    "points = np.zeros((len(bins) - 1, len(local_similarity_results)))\n",
    "for i in range(len(local_similarity_results)):\n",
    "    points[:, i], _ = np.histogram(local_similarity_results[i], bins=bins, density=True)\n",
    "y_data, bins = np.histogram(np.array(local_similarity_results).flatten(), bins=bin_num, density=True)\n",
    "y_err = np.std(points, axis=1)  # Compute error\n",
    "y_err /= np.sum(y_data)  # Normalize\n",
    "y_data /= np.sum(y_data)  # Normalize\n",
    "\n",
    "plt.fill_between(bins[:-1], y_data - y_err, y_data + y_err, color=colors[1], hatch=\"XX\", edgecolor=\"k\",\n",
    "                alpha=0.3, label=\"Local Model ± Err\",zorder=4) \n",
    "pl1, = plt.plot(bins[:-1], y_data, lw=3, alpha=0.6,color=colors[1], zorder=4)\n",
    "pl2, = plt.plot(bins[:-1], y_data, 'k',marker=\"^\", ls='dashdot', ms=8, mec=\"white\",\n",
    "                linewidth=1, mfc=colors[1], zorder=5,label=\"Local Model ± Err\")\n",
    "\n",
    "\n",
    "# random:::::::::::::::::::::::::::::::::::::::::::::::\n",
    "bin_num = sturges(len(np.mean(rand_overlap_results, axis=0)))\n",
    "y_data, bins = np.histogram(np.mean(rand_similarity_results, axis=0), bins=rand_bin, density=True)\n",
    "points = np.zeros((len(bins) - 1, len(rand_similarity_results)))\n",
    "for i in range(len(rand_similarity_results)):\n",
    "    points[:, i], _ = np.histogram(rand_similarity_results[i], bins=rand_bin, density=True)\n",
    "y_err = np.std(points, axis=1)  # Compute error\n",
    "y_err /= np.sum(y_data)  # Normalize\n",
    "y_data /= np.sum(y_data)  # Normalize\n",
    "\n",
    "plt.fill_between(bins[:-1], y_data - y_err, y_data + y_err, alpha=0.3, color=colors[3],\n",
    "                label=\"No Preference ± Err\",zorder=2)\n",
    "pr1, = plt.plot(bins[:-1], y_data, lw=4, zorder=10,alpha=0.6,color=colors[3])\n",
    "pr2, = plt.plot(bins[:-1], y_data, 'k',marker=\"d\", ls='dotted', ms=8, mec=\"white\",\n",
    "                linewidth=1, mfc=colors[3], zorder=10,label=\"No Preference ± Err\")\n",
    "\n",
    "\n",
    "# data:::::::::::::::::::::::::::::::::\n",
    "bin_num = sturges(len(overlaps))\n",
    "y_data, bins = np.histogram(similarities, bins=bin_num, density=True)\n",
    "y_data /= np.sum(y_data)  # Normalize\n",
    "pd1, = plt.plot(bins[:-1], y_data, lw=4, color='k', zorder=10,alpha=0.6, label='Retweet Data')\n",
    "pd2, = plt.plot(bins[:-1], y_data, 'white',marker=\"o\", ls=(0,(1,5)), ms=8, mec=\"white\",\n",
    "                linewidth=1, mfc=\"black\", zorder=10,label=\"Retweet Data\")\n",
    "\n",
    "\n",
    "# Titles and Labels\n",
    "plt.xlim(right=1, left=0)\n",
    "plt.ylim(bottom=0)\n",
    "plt.xlabel(r\"$\\langle S_i^{h,g} \\rangle$\", fontsize=12)\n",
    "plt.ylabel(r\"$P(\\langle S_i^{h,g} \\rangle)$\", fontsize=12)\n",
    "\n",
    "plt.legend()\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend([ tuple(handles[0:2]), tuple(handles[2:4]), tuple(handles[4:6]), tuple(handles[6:8])], labels[::2], handlelength=3,\n",
    "          handler_map={tuple: HandlerTuple(ndivide=1)})\n",
    "\n",
    "# plt.savefig(\"Similarities\"+arch.generate_magic_number()+\".png\",dpi=200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = {'0': similarities, '1': global_similarity_results[0], \n",
    "           '2': local_similarity_results[0], '3': rand_similarity_results[0]}\n",
    "names = {'0': 'Data', '1': 'Global', '2': 'Local', '3': 'No Preference'}\n",
    "overalp = {'0': overlaps, '1': global_overlap_results[0], \n",
    "           '2': local_overlap_results[0], '3': rand_overlap_results[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Modified Weighted Jaccard Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stat test of overlaps\n",
    "\n",
    "# Dictionary\n",
    "overalp = {'0': overlaps, '1': global_overlap_results[0], \n",
    "           '2': local_overlap_results[0], '3': rand_overlap_results[0]}\n",
    "names = {'0': 'Data', '1': 'Global', '2': 'Local', '3': 'No Preference'}\n",
    "\n",
    "# Initialize matrices\n",
    "stat_matrix = np.zeros((4, 4))\n",
    "p_value_matrix = np.zeros((4, 4))\n",
    "\n",
    "# Calculate KS test for each pair\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        stat, p_value = ks_2samp(overalp[str(i)], overalp[str(j)])\n",
    "        stat_matrix[i, j] = stat\n",
    "        p_value_matrix[i, j] = p_value\n",
    "\n",
    "# Plotting the results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot KS Statistic\n",
    "cax1 = ax1.imshow(stat_matrix, cmap='Blues', aspect='auto',\n",
    "                  norm=Normalize(vmin=0, vmax=stat_matrix.max()))\n",
    "ax1.set_title('Modified weighted Jaccard index')\n",
    "fig.colorbar(cax1, ax=ax1)\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        color = 'white' if stat_matrix[i,j] > stat_matrix.max() / 2 else 'black'\n",
    "        ax1.text(j, i, f'{stat_matrix[i, j]:.3f}', ha='center', va='center', color=color)\n",
    "\n",
    "# Plot P-Values\n",
    "cax2 = ax2.imshow(p_value_matrix, cmap='Blues', aspect='auto',\n",
    "                  norm=Normalize(vmin=0, vmax=p_value_matrix.max()))\n",
    "ax2.set_title('P-Values')\n",
    "fig.colorbar(cax2, ax=ax2)\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        color = 'white' if p_value_matrix[i,j] > p_value_matrix.max() / 2 else 'black'\n",
    "        ax2.text(j, i, f'{p_value_matrix[i, j]:.3e}', ha='center', va='center', color=color)\n",
    "\n",
    "# Labels and layout\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.set_xticks(range(4))\n",
    "    ax.set_yticks(range(4))\n",
    "    ax.set_xticklabels([names[str(i)] for i in range(4)])\n",
    "    ax.set_yticklabels([names[str(i)] for i in range(4)])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('KS_overlap.png', dpi=200)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Cosine Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stat test of cosine similarities\n",
    "\n",
    "# Dictionary\n",
    "sims = {'0': similarities, '1': global_similarity_results[0], \n",
    "           '2': local_similarity_results[0], '3': rand_similarity_results[0]}\n",
    "names = {'0': 'Data', '1': 'Global', '2': 'Local', '3': 'No Preference'}\n",
    "\n",
    "# Initialize matrices\n",
    "stat_matrix = np.zeros((4, 4))\n",
    "p_value_matrix = np.zeros((4, 4))\n",
    "\n",
    "# Calculate KS test for each pair\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        stat, p_value = ks_2samp(sims[str(i)], sims[str(j)])\n",
    "        stat_matrix[i, j] = stat\n",
    "        p_value_matrix[i, j] = p_value\n",
    "\n",
    "# Plotting the results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot KS Statistic\n",
    "cax1 = ax1.imshow(stat_matrix, cmap='Blues', aspect='auto',\n",
    "                  norm=Normalize(vmin=0, vmax=stat_matrix.max()))\n",
    "ax1.set_title('Cosine similarities')\n",
    "fig.colorbar(cax1, ax=ax1)\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        color = 'white' if stat_matrix[i,j] > stat_matrix.max() / 2 else 'black'\n",
    "        ax1.text(j, i, f'{stat_matrix[i, j]:.3f}', ha='center', va='center', color=color)\n",
    "\n",
    "# Plot P-Values\n",
    "cax2 = ax2.imshow(p_value_matrix, cmap='Blues', aspect='auto',\n",
    "                  norm=Normalize(vmin=0, vmax=p_value_matrix.max()))\n",
    "ax2.set_title('P-Values')\n",
    "fig.colorbar(cax2, ax=ax2)\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        color = 'white' if p_value_matrix[i,j] > p_value_matrix.max() / 2 else 'black'\n",
    "        ax2.text(j, i, f'{p_value_matrix[i, j]:.3e}', ha='center', va='center', color=color)\n",
    "\n",
    "# Labels and layout\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.set_xticks(range(4))\n",
    "    ax.set_yticks(range(4))\n",
    "    ax.set_xticklabels([names[str(i)] for i in range(4)])\n",
    "    ax.set_yticklabels([names[str(i)] for i in range(4)])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('KS_sim.png',dpi=200)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
